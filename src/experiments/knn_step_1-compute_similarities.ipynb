{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute KNN similarities\n",
    "\n",
    "Computes similarities between each pair of dates based on how skillfully the history of one date predicts the history of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package loading\n",
    "\n",
    "# Autoreload packages that are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plotting magic\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4\n",
    "import time\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"experiments\":\n",
    "    os.chdir(os.path.join(\"..\",\"..\"))\n",
    "\n",
    "# Adds 'experiments' folder to path to load experiments_util\n",
    "sys.path.insert(0, 'src/experiments')\n",
    "# Load general utility functions\n",
    "from experiments_util import *\n",
    "# Load functionality for fitting and predicting\n",
    "from fit_and_predict import *\n",
    "# Load functionality for evaluation\n",
    "from skill import *\n",
    "\n",
    "## Prepare experimental results directory structure\n",
    "\n",
    "# Set hindcast_year to None to obtain forecasts and to a specific year to obtain hindcasts\n",
    "hindcast_year = None\n",
    "\n",
    "# Choose the name of this experiment\n",
    "experiment = \"knn\"\n",
    "if hindcast_year is not None:\n",
    "    experiment = \"knn-hindcast_{}\".format(hindcast_year) ### For hindcasts\n",
    "    \n",
    "# Name of cache directory for storing non-submission-date specific\n",
    "# intermediate files\n",
    "cache_dir = os.path.join('results', experiment, 'shared')\n",
    "# if cache_dir doesn't exist, create it\n",
    "if not os.path.isdir(cache_dir):\n",
    "    os.makedirs(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable to be predicted\n",
    "#gt_id = \"contest_precip\" # \"contest_precip\" or \"contest_tmp2m\"\n",
    "gt_id = \"contest_tmp2m\" # \"contest_precip\" or \"contest_tmp2m\"\n",
    "\n",
    "## Process inputs\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'prate'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'prate_anom'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ground truth cosine similarities between pairs of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting contest_tmp2m_shiftNone with anomalies\n",
      "Elapsed: 11.048771619796753s\n",
      "Elapsed time: 3.671401 seconds.\n",
      "\n",
      "Elapsed time: 0.152220 seconds.\n",
      "\n",
      "Elapsed time: 6.189561 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if experiment == \"knn\":\n",
    "    # Non-hindcast version\n",
    "    # Load ground truth anomalies\n",
    "    anoms = get_lat_lon_date_features(anom_ids = [gt_id], first_year=get_first_year(gt_id))\n",
    "else:\n",
    "    # Hindcast version\n",
    "    tic()\n",
    "    # Load ground truth data\n",
    "    anoms = get_lat_lon_date_features(gt_ids = [gt_id], first_year=get_first_year(gt_id))\n",
    "    # Load ground truth data climatology\n",
    "    climatology = get_climatology(gt_id)\n",
    "    # Identify ground truth data from this hold out year\n",
    "    first_holdout_date = datetime(month=4, day=18, year=hindcast_year)\n",
    "    last_holdout_date = datetime(month=4, day=17, year=hindcast_year+1)\n",
    "    gt_col = get_measurement_variable(gt_id)\n",
    "    holdout = anoms.loc[(anoms.start_date >= first_holdout_date)\n",
    "                        &(anoms.start_date <= last_holdout_date), \n",
    "                        ['lat','lon','start_date',gt_col]]\n",
    "    # Merge the hindcast year ground truth data into climatology dataframe\n",
    "    climatology = pd.merge(\n",
    "        holdout[[gt_col]], climatology,\n",
    "        left_on=[holdout.lat, holdout.lon, holdout.start_date.dt.month,\n",
    "                 holdout.start_date.dt.day],\n",
    "        right_on=[climatology.lat, climatology.lon,\n",
    "                  climatology.start_date.dt.month,\n",
    "                  climatology.start_date.dt.day],\n",
    "        how='left', suffixes=('', '_clim'))\n",
    "    clim_col = gt_col+\"_clim\"\n",
    "    # Remove the influence of hindcast year from 30-year climatology average\n",
    "    years_in_clim = 30\n",
    "    climatology[clim_col] = (climatology[clim_col]*years_in_clim - climatology[gt_col])/(years_in_clim-1)\n",
    "    # Merge modified climatology into dataset\n",
    "    anoms = pd.merge(anoms, climatology[[clim_col]],\n",
    "                      left_on=['lat', 'lon', anoms.start_date.dt.month,\n",
    "                               anoms.start_date.dt.day],\n",
    "                      right_on=[climatology.lat, climatology.lon,\n",
    "                                climatology.start_date.dt.month,\n",
    "                                climatology.start_date.dt.day],\n",
    "                      how='left', suffixes=('', '_clim'))\n",
    "    # Compute ground-truth anomalies using new climatology\n",
    "    anom_col = gt_col+\"_anom\"\n",
    "    anoms[anom_col] = anoms[gt_col] - anoms[clim_col]\n",
    "    toc()\n",
    "    \n",
    "# Drop unnecessary columns\n",
    "anoms = anoms.loc[:,['lat','lon','start_date',anom_col]]\n",
    "# Pivot dataframe to have one row per start date and one column per (lat,lon)\n",
    "tic(); anoms = anoms.set_index(['lat','lon','start_date']).unstack(['lat','lon']); toc()\n",
    "# Drop start dates that have no measurements (e.g., leap days, which have no climatology)\n",
    "anoms = anoms.dropna(axis='index', how='all')\n",
    "# Normalize each start_date's measurements by its Euclidean norm\n",
    "tic()\n",
    "norms = np.sqrt(np.square(anoms).sum(axis=1))\n",
    "anoms = anoms.divide(norms, axis=0)\n",
    "toc()\n",
    "# Compute the cosine similarity between each pair of dates by computing all inner products\n",
    "tic(); gt_cosines = anoms.dot(anoms.transpose()); toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each date is represented by its past_days most recent observed measurements (i.e., \n",
    "# the past_days most recent measurements at least start_delta days before the date).\n",
    "# The similarity of two dates is the average cosine similarity their past_days\n",
    "# associated measurements.\n",
    "\n",
    "# The number of past days that should contribute to measure of similarity\n",
    "past_days = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity measure between pairs of target dates assuming start_delta = 0\n",
    "That is, assuming that we have access to the ground truth measurement with start date equal to the target date.\n",
    "Later we will shift by start_delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3.593677 seconds.\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 Elapsed time: 100.443004 seconds.\n",
      "\n",
      "Saving similarities0 to  results/knn/shared/similarities0-contest_tmp2m-days60.h5 None\n",
      "Elapsed time: 24.187716 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if base similarities have been computed previously\n",
    "regen_similarities0 = True\n",
    "similarities0_file = os.path.join(\n",
    "    cache_dir,'similarities0-{}-days{}.h5'.format(gt_id,past_days))\n",
    "if regen_similarities0 or not os.path.isfile(similarities0_file):\n",
    "    # Initially incorporate unshifted cosine similarities \n",
    "    # (representing the cosine similarity of the first past day)\n",
    "    tic()\n",
    "    similarities0 = gt_cosines.copy()\n",
    "    toc()\n",
    "\n",
    "    # Now, for each remaining past day, sum over additionally shifted measurements\n",
    "    # NOTE: this has the effect of ignoring (i.e., skipping over) dates that don't \n",
    "    # exist in gt_cosines like leap days\n",
    "    tic()\n",
    "    for m in range(1,past_days):\n",
    "        similarities0 += gt_cosines.shift(m, axis='rows').shift(m, axis='columns')\n",
    "        sys.stdout.write(str(m)+' ')\n",
    "    toc()\n",
    "\n",
    "    # Normalize similarities by number of past days\n",
    "    similarities0 /= past_days\n",
    "    # Write similarities0 to file\n",
    "    print(\"Saving similarities0 to \", similarities0_file); tic()\n",
    "    similarities0.to_hdf(similarities0_file, key=\"data\", mode=\"w\"); toc()\n",
    "else:\n",
    "    # Read base similarities from disk\n",
    "    print(\"Reading similarities0 from \", similarities0_file, tic())\n",
    "    similarities0 = pd.read_hdf(similarities0_file); toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction horizon\n",
    "target_horizon = \"56w\" # \"34w\" or \"56w\"\n",
    "\n",
    "# Only use measurements available this many days prior to \n",
    "# official contest submission date\n",
    "days_early = 365 - (14 + get_forecast_delta(target_horizon, days_early = 0)) \n",
    "\n",
    "## Process inputs\n",
    "\n",
    "# Number of days between start date of most recently observed measurement\n",
    "# (2 weeks to observe complete measurement) and start date of target period \n",
    "# (2 or 4 weeks plus days early days ahead)\n",
    "aggregation_days = 14\n",
    "start_delta = (aggregation_days + \n",
    "               get_forecast_delta(target_horizon, days_early = days_early))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift similarities by start_delta\n",
    "The rows and columns of similarities represent target dates, and the similarities are now based on ground truth measurements from start_delta days prior to each target date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 4.042113 seconds.\n",
      "\n",
      "Elapsed time: 2.154238 seconds.\n",
      "\n",
      "Elapsed time: 0.710397 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The earliest measurement available is from start_delta days prior to target day, \n",
    "# so shift rows and columns of similarities by start_delta and extend index accordingly\n",
    "# NOTE: For some reason, shifting columns doesn't extend column index, so I'm transposing and shifting\n",
    "# rows\n",
    "tic()\n",
    "similarities = similarities0.shift(start_delta, axis='rows', freq='D').transpose().shift(start_delta, axis='rows', freq='D')\n",
    "toc()\n",
    "# Index extension has the side effect of creating leap days (e.g., 2012-02-29) and removing \n",
    "# the date start_delta days later (e.g., datetime.date(2012,2,29) + timedelta(start_delta))\n",
    "# Add one day to each date in the range [datetime.date(2012,2,29), \n",
    "# datetime.date(2012,2,29) + timedelta(start_delta)) to remove leap days\n",
    "def fix_date(date):\n",
    "    if date.is_leap_year:\n",
    "        # Identify the affected dates in this current date's year\n",
    "        affected_dates = pd.date_range('{}-02-29'.format(date.year), periods=start_delta, freq='D')\n",
    "    elif date.replace(year=date.year-1).is_leap_year:\n",
    "        # Identify the affected dates starting from prior year\n",
    "        affected_dates = pd.date_range('{}-02-29'.format(date.year-1), periods=start_delta, freq='D')\n",
    "    else:\n",
    "        # Only modify leap year dates and dates following leap year\n",
    "        return date\n",
    "    # Shift date by 1 day if affected\n",
    "    return date + timedelta(1) if date in affected_dates else date\n",
    "tic()\n",
    "new_index = [fix_date(date) for date in similarities.index]\n",
    "toc()\n",
    "tic()\n",
    "similarities = similarities.reindex(new_index)\n",
    "similarities.columns = new_index\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict similarities to viable neighbors\n",
    "Viable neighbors are those with available ground truth data (as evidenced by anoms or gt_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving viable_similarities to  results/knn/shared/viable_similarities-contest_tmp2m-56w-days60-early323.h5\n",
      "Elapsed time: 25.193596 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if viable similarities have been computed previously\n",
    "regen_viable_similarities = True\n",
    "viable_similarities_file = os.path.join(\n",
    "    cache_dir,'viable_similarities-{}-{}-days{}-early{}.h5'.format(gt_id,target_horizon,past_days,days_early))\n",
    "if regen_viable_similarities or not os.path.isfile(viable_similarities_file):\n",
    "    viable_similarities = similarities[similarities.index.isin(gt_cosines.index)]\n",
    "    print(\"Saving viable_similarities to \", viable_similarities_file); tic()\n",
    "    viable_similarities.to_hdf(viable_similarities_file, key=\"data\", mode=\"w\"); toc()\n",
    "else:\n",
    "    # Read viable similarities from disk\n",
    "    print(\"Reading viable similarities from \", viable_similarities_file); tic()\n",
    "    viable_similarities = pd.read_hdf(viable_similarities_file); toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
