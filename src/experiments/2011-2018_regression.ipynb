{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression for 2011-2018 target dates\n",
    "\n",
    "Carry out regression experiment for a fixed set of predictors and all 2011-2018 target dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Autoreload packages that are modified\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Plotting magic\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4\n",
    "import time\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "# Ensure that working directory is forecast_rodeo\n",
    "if os.path.basename(os.getcwd()) == \"experiments\":\n",
    "    # Navigate to forecast_rodeo\n",
    "    os.chdir(os.path.join(\"..\",\"..\"))\n",
    "if os.path.basename(os.getcwd()) != \"forecast_rodeo\":\n",
    "    raise Exception(\"You must be in the forecast_rodeo folder\")\n",
    "\n",
    "# Adds 'experiments' folder to path to load experiments_util\n",
    "sys.path.insert(0, 'src/experiments')\n",
    "# Load general utility functions\n",
    "from experiments_util import *\n",
    "# Load functionality for fitting and predicting\n",
    "from fit_and_predict import *\n",
    "# Load functionality for evaluation\n",
    "from skill import *\n",
    "# Load stepwise utility functions\n",
    "from stepwise_util import *\n",
    "\n",
    "# Experiment name\n",
    "experiment = \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Choose target\n",
    "#\n",
    "gt_id = \"contest_tmp2m\" # \"contest_precip\" or \"contest_tmp2m\"\n",
    "target_horizon = \"56w\" # \"34w\" or \"56w\"\n",
    "\n",
    "#\n",
    "# Set variables based on target choice\n",
    "#\n",
    "\n",
    "# Identify measurement variable name\n",
    "measurement_variable = get_measurement_variable(gt_id) # 'tmp2m' or 'precip'\n",
    "\n",
    "# column names for gt_col, clim_col and anom_col \n",
    "gt_col = measurement_variable\n",
    "clim_col = measurement_variable+\"_clim\"\n",
    "anom_col = get_measurement_variable(gt_id)+\"_anom\" # 'tmp2m_anom' or 'precip_anom'\n",
    "\n",
    "# anom_inv_std_col: column name of inverse standard deviation of anomalies for each start_date\n",
    "anom_inv_std_col = anom_col+\"_inv_std\"\n",
    "\n",
    "# Name of knn columns\n",
    "knn_cols = [\"knn\"+str(ii) for ii in range(1,21)]\n",
    "\n",
    "#\n",
    "# Create list of official contest submission dates in YYYYMMDD format\n",
    "#\n",
    "submission_dates = [datetime(y,4,18)+timedelta(14*i) for y in range(2011,2018) for i in range(26)]\n",
    "submission_dates = ['{}{:02d}{:02d}'.format(date.year, date.month, date.day) for date in submission_dates]\n",
    "submission_dates = [datetime.strptime(str(d), \"%Y%m%d\") for d in submission_dates]\n",
    "submission_dates = pd.Series(submission_dates)\n",
    "\n",
    "#\n",
    "# Create list of target dates corresponding to submission dates in YYYYMMDD format\n",
    "#\n",
    "target_dates = pd.Series([get_target_date('{}{:02d}{:02d}'.format(date.year, date.month, date.day), target_horizon) for date in submission_dates])\n",
    "\n",
    "# Find all unique target day-month combinations\n",
    "target_day_months = pd.DataFrame({'month' : target_dates.dt.month, \n",
    "                                  'day': target_dates.dt.day}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knn1', 'knn2', 'knn3', 'knn4', 'knn5', 'knn6', 'knn7', 'knn8', 'knn9', 'knn10', 'knn11', 'knn12', 'knn13', 'knn14', 'knn15', 'knn16', 'knn17', 'knn18', 'knn19', 'knn20', 'ones', 'tmp2m_shift43_anom', 'tmp2m_shift86_anom', 'tmp2m_shift365_anom']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Choose regression parameters\n",
    "#\n",
    "# Record standard settings of these parameters\n",
    "setting = \"knn_regression\"\n",
    "if setting == \"knn_regression\":\n",
    "    base_col = clim_col\n",
    "    # Number of KNN neighbors to use in regression\n",
    "    num_nbrs = 1 if gt_id.endswith('precip') else 20\n",
    "    x_cols = ['knn'+str(nbr) for nbr in range(1,num_nbrs+1)] + ['ones']\n",
    "    # Construct fixed lag anomaly variable names\n",
    "    lags = (['43', '86'] if target_horizon == '56w' else ['29', '58']) + ['365']\n",
    "    x_cols = x_cols + [measurement_variable+'_shift'+lag+'_anom' for lag in lags] \n",
    "    # Determine margin for local regression\n",
    "    margin_in_days = 56 if gt_id.endswith('precip') else None\n",
    "    # columns to group by when fitting regressions (a separate regression\n",
    "    # is fit for each group); use ['ones'] to fit a single regression to all points\n",
    "    group_by_cols = ['lat', 'lon']\n",
    "    # anom_scale_col: multiply anom_col by this amount prior to prediction\n",
    "    # (e.g., 'ones' or anom_inv_std_col)\n",
    "    anom_scale_col = anom_inv_std_col\n",
    "    # pred_anom_scale_col: multiply predicted anomalies by this amount\n",
    "    # (e.g., 'ones' or anom_inv_std_col)\n",
    "    pred_anom_scale_col = anom_scale_col\n",
    "elif setting == \"stepwise_no_model_selection\":\n",
    "    base_col = clim_col\n",
    "    x_cols = default_stepwise_candidate_predictors(gt_id, target_horizon, hindcast=False) + ['knn1']\n",
    "    margin_in_days = 56\n",
    "    group_by_cols = ['lat', 'lon']\n",
    "    anom_scale_col = anom_inv_std_col\n",
    "    pred_anom_scale_col = anom_scale_col\n",
    "\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading date and lat lon date data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File results/regression/contest_tmp2m_56w/date_data-contest_tmp2m_56w.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8b16e2f17475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading date and lat lon date data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_horizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"date_data-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgt_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_horizon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdate_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m lat_lon_date_data = pd.read_hdf(\n",
      "\u001b[0;32m~/.conda/envs/geo/lib/python3.6/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             raise compat.FileNotFoundError(\n\u001b[0;32m--> 366\u001b[0;31m                 'File {path} does not exist'.format(path=path_or_buf))\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File results/regression/contest_tmp2m_56w/date_data-contest_tmp2m_56w.h5 does not exist"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Default regression parameter values\n",
    "#\n",
    "# choose first year to use in training set\n",
    "first_train_year = 1948 if gt_id == 'contest_precip' else 1979\n",
    "# specify regression model\n",
    "fit_intercept = False\n",
    "model = linear_model.LinearRegression(fit_intercept=fit_intercept)\n",
    "\n",
    "#\n",
    "# Prepare target and feature data\n",
    "#\n",
    "relevant_cols = set(x_cols+[base_col,clim_col,anom_col,'sample_weight','target',\n",
    "                    'start_date','lat','lon','year','ones']+group_by_cols)\n",
    "# Create dataset with relevant columns only; otherwise the dataframe is too big\n",
    "tic()\n",
    "print(\"Loading date and lat lon date data\")\n",
    "data_dir = os.path.join(\"results\", experiment, \"shared\", gt_id + \"_\" + target_horizon)\n",
    "date_data = pd.read_hdf(os.path.join(data_dir, \"date_data-\" + gt_id + \"_\" + target_horizon + \".h5\"))\n",
    "date_data['year'] = date_data.start_date.dt.year\n",
    "lat_lon_date_data = pd.read_hdf(\n",
    "    os.path.join(data_dir, \"lat_lon_date_data-\" + gt_id + \"_\" + target_horizon + \".h5\"))\n",
    "toc()\n",
    "# Restrict data to years >= first_train_year\n",
    "tic()\n",
    "print(\"Restricting data to years >= {}\".format(first_train_year))\n",
    "lat_lon_date_data = lat_lon_date_data.loc[lat_lon_date_data.start_date.dt.year >= first_train_year]\n",
    "date_data = date_data.loc[date_data.year >= first_train_year]\n",
    "toc()\n",
    "# Drop rows with missing values for any relevant column \n",
    "tic()\n",
    "print(\"Dropping rows with missing values for any relevant columns\")\n",
    "relevant_lat_lon_date_cols = list(set(lat_lon_date_data.columns.tolist()) & relevant_cols)\n",
    "lat_lon_date_data.dropna(subset = relevant_lat_lon_date_cols, inplace = True)\n",
    "relevant_date_cols = list(set(date_data.columns.tolist()) & relevant_cols)\n",
    "date_data.dropna(subset = relevant_date_cols, inplace = True)\n",
    "toc()\n",
    "# Add supplementary columns\n",
    "tic()\n",
    "print(\"Adding supplementary columns\")\n",
    "lat_lon_date_data['anom_inv_sqrt_2nd_mom'] = 1.0/np.sqrt(\n",
    "    lat_lon_date_data.groupby('start_date')[anom_col].transform('mean')**2\n",
    "    + lat_lon_date_data.groupby('start_date')[anom_col].transform('var',ddof=0))\n",
    "lat_lon_date_data['ones'] = 1.0\n",
    "lat_lon_date_data['zeros'] = 0.0\n",
    "# To minimize the mean-squared error between predictions of the form\n",
    "# (f(x_cols) + base_col - clim_col) * pred_anom_scale_col\n",
    "# and a target of the form anom_col * anom_scale_col, we will\n",
    "# estimate f using weighted least squares with datapoint weights\n",
    "# pred_anom_scale_col^2 and effective target variable \n",
    "# anom_col * anom_scale_col / pred_anom_scale_col + clim_col - base_col\n",
    "lat_lon_date_data['sample_weight'] = lat_lon_date_data[pred_anom_scale_col]**2\n",
    "lat_lon_date_data['target'] = (lat_lon_date_data[clim_col] - lat_lon_date_data[base_col] + \n",
    "                               lat_lon_date_data[anom_col] * lat_lon_date_data[anom_scale_col] / \n",
    "                              (lat_lon_date_data[pred_anom_scale_col]+(lat_lon_date_data[pred_anom_scale_col]==0)))\n",
    "toc()\n",
    "\n",
    "# Load KNN data\n",
    "tic()\n",
    "print(\"Loading KNN data\")\n",
    "past_days = 60\n",
    "days_early = 337 if target_horizon == \"34w\" else 323\n",
    "max_nbrs = 20\n",
    "knn_dir = os.path.join(\"data\", \"dataframes\")\n",
    "knn_data = pd.read_hdf(\n",
    "    os.path.join(knn_dir, \n",
    "                 \"knn-{}-{}-days{}-early{}-maxnbrs{}.h5\".format(\n",
    "                     gt_id, target_horizon, past_days, days_early, max_nbrs)))\n",
    "relevant_knn_cols = list(set(knn_cols) & relevant_cols)\n",
    "# Divide knn anomalies by std dev across grid cells\n",
    "knn_data[relevant_knn_cols] /= knn_data.groupby([\"start_date\"])[relevant_knn_cols].transform('std')\n",
    "toc()\n",
    "\n",
    "# Restrict data to relevant columns\n",
    "tic()\n",
    "print(\"Merge datasets\")\n",
    "relevant_lat_lon_date_cols = list(set(lat_lon_date_data.columns.tolist()) & relevant_cols)\n",
    "data = lat_lon_date_data.loc[:, relevant_lat_lon_date_cols]\n",
    "relevant_knn_cols = list(set(knn_data.columns.tolist()) & relevant_cols)\n",
    "data = pd.merge(data, knn_data[relevant_knn_cols],\n",
    "                on=[\"start_date\",\"lat\",\"lon\"], how=\"left\")\n",
    "data = pd.merge(data, date_data[relevant_date_cols],\n",
    "                on=\"start_date\", how=\"left\")\n",
    "print(data.head())\n",
    "del lat_lon_date_data\n",
    "del knn_data\n",
    "del date_data\n",
    "toc()\n",
    "\n",
    "# Print warning if not all x columns were included\n",
    "s = [x for x in x_cols if x not in data.columns.tolist()]\n",
    "if s:\n",
    "    print(\"These x columns were not found:\")\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 8\n",
    "# For the target (day, month) combination, fit leave-one-year regression model \n",
    "# on training set subsetted to relevant margin and generate predictions for each \n",
    "# held-out year\n",
    "prediction_func = rolling_linear_regression_wrapper\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "generic_year = 2011\n",
    "for day_month in target_day_months.itertuples():\n",
    "    # Get target date on generic as a datetime object\n",
    "    target_date_obj = datetime.strptime('{}{:02d}{:02d}'.format(\n",
    "        generic_year, day_month.month, day_month.day), \"%Y%m%d\")\n",
    "    print 'day_month={}, target={}'.format(day_month, target_date_obj)\n",
    "    # Get number of days between start date of observation period used for prediction\n",
    "    # (2 weeks ahead) and start date of target period (2 or 4 weeks ahead) + 1 day do \n",
    "    # to practical constraints of submission\n",
    "    start_delta = get_start_delta(target_horizon) # 29 or 43\n",
    "    # Create template for held-out years: each held-out year will run from \n",
    "    # last_train_date + 1 on that year (inclusive) through last_train_date\n",
    "    # of the next year (inclusive)\n",
    "    last_train_date = target_date_obj - timedelta(start_delta)\n",
    "    if margin_in_days is not None:\n",
    "        tic()\n",
    "        sub_data = month_day_subset(data, target_date_obj, margin_in_days)\n",
    "        toc()\n",
    "    else:\n",
    "        sub_data = data\n",
    "    tic()\n",
    "    preds = apply_parallel(sub_data.groupby(group_by_cols),\n",
    "                           prediction_func, num_cores,\n",
    "                           x_cols=x_cols, \n",
    "                           base_col=base_col, \n",
    "                           clim_col=clim_col, \n",
    "                           anom_col=anom_col, \n",
    "                           last_train_date=last_train_date)\n",
    "    preds = preds.reset_index() \n",
    "    # Only keep the predictions from the target day and month\n",
    "    preds = preds[(preds.start_date.dt.day == target_date_obj.day) & \n",
    "                  (preds.start_date.dt.month == target_date_obj.month)]\n",
    "    # Concatenate predictions\n",
    "    all_preds = pd.concat([all_preds, preds])\n",
    "    toc()\n",
    "    #---------------\n",
    "    # Evaluate only on target dates                                                        \n",
    "    #---------------\n",
    "    tic()\n",
    "    skills = get_col_skill(\n",
    "        all_preds[all_preds.start_date.isin(target_dates)], \n",
    "        \"truth\", \"forecast\", time_average = False)\n",
    "    print \"running mean skill = {}\".format(skills.mean())\n",
    "    toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# Evaluate on all target dates                                                        \n",
    "#---------------\n",
    "tic()\n",
    "skills = get_col_skill(\n",
    "    all_preds[all_preds.start_date.isin(target_dates)], \n",
    "    \"truth\", \"forecast\", time_average = False)\n",
    "print \"overall mean skill = {}\".format(skills.mean())\n",
    "#---------------\n",
    "# Evaluate on contest year                                                      \n",
    "#---------------\n",
    "skills = get_col_skill(\n",
    "    all_preds[all_preds.start_date.isin(target_dates) & (all_preds.start_date >= \n",
    "              get_target_date(\"20170418\", target_horizon))], \n",
    "    \"truth\", \"forecast\", time_average = False)\n",
    "print \"2017-2018 mean skill = {}\".format(skills.mean())\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Save target date predictions to file\n",
    "#\n",
    "\n",
    "# Ensure hash randomization turned off for reproducibility\n",
    "PYTHONHASHSEED=0\n",
    "# Define a compact string identifier for experiment parameters\n",
    "param_str = str(abs(hash((base_col, frozenset(x_cols), margin_in_days, frozenset(group_by_cols),\n",
    "                         anom_scale_col, pred_anom_scale_col))))\n",
    "# Create directory for storing results\n",
    "outdir = os.path.join('results',experiment,'2011-2018',gt_id+'_'+target_horizon,param_str)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "# Write predictions to file; record the dependency on the type of knn features integrated\n",
    "# into the feature set in the file name\n",
    "preds_file = os.path.join(\n",
    "    outdir,'preds-{}-{}-days{}-early{}.h5'.format(gt_id,target_horizon,past_days,days_early))\n",
    "print \"Saving predictions to \"+preds_file; tic()\n",
    "all_preds[all_preds.start_date.isin(target_dates)].to_hdf(preds_file, key=\"data\", mode=\"w\"); toc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
